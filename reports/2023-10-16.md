# Activities
* Fixed FGSM implementation because noticed the clamp was clamping to the wrong range of numbers

# Issues
* N/A

# Plans
* see if cleverhans library can be implemented
* start implementing CW + PGD
  * [PGD Article](https://towardsdatascience.com/know-your-enemy-7f7c5038bdf3#:~:text=PGD%20attempts%20to%20find%20the,amount%20referred%20to%20as%20epsilon%20.)
* read this paper on an iterative FGSM method:
   * [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)

# Paper Summaries
## [Test-time Detection and Repair of Adversarial Samples via Masked Autoencoder](https://arxiv.org/pdf/2303.12848.pdf)
* Test-time defenses: defenses to evasion attacks that detect at test-time
* They propose DRAM: using MAE (masked autoencoder) they detect and repair adversarial attacks
    * MAE: A Masked Autoencoder is a type of artificial neural network architecture used in unsupervised machine learning and neural network-based dimensionality reduction techniques. It is designed to learn a compact representation of the input data by encoding the data into a lower-dimensional representation and then decoding it back into its original form
    * detect: "We leverage MAE to detect adversarial samples generated from potentially unforeseen attacks via a statistic-based test on the MAE loss." aka looking at reconstruction loss
    * repair: "minimizing MAE loss at run-time"
* Results show that DRAM achieves an average 82% detection score on eight previously unseen attack
* method for attack repair improved the robust accuracy of ResNet50 by up to 40% and 8% under standard and robust training after repair.

## [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)
* Created the targeted evasion attack CW
* Three approaches in targeted attack (targeted attack targets certain classes while untargeted just aim for the class to be incorrect):
  * Average Case: select the target class uniformly at random among the labels that are not the correct label.
  * Best Case: perform the attack against all incorrect classes, and report the target class that was least difficult to attack.
  * Worst Case: perform the attack against all incorrect classes, and report the target class that was most difficult
to attack.
* They use 3 distance metrics (all L norms) Distance metrics are usually used in order to limit the potential noticeable perturbation to a human viewer.
  * L_0 -> number of pixels unaltered in the formation of an adversarial example
  * L_2 -> Euclidean norm between the adversary example and the un-perturbed image
  * L_inf -> measures max change of any of the pixels between the adversary example and the un-perturbed image
* Tested it on MNIST, CIFAR-10, ImageNet
  
  <img width="263" alt="image" src="https://github.com/christymarc/thesis/assets/70297740/e17b3099-c991-458a-ad63-aad758a27263">

* for imagenet, used the pre-trained Inception v3 network which achieves 96% top-5 accuracy
* They look at a variety of functions that meet the constraints of their objective function f such that C(x + δ) = t if and only if f(x + δ) ≤ 0.
* Then they consider these functions on three different methods of box constraint (aka constraint on δ: we must have 0 ≤ xi +δi ≤ 1 for all i):
  * projected gradient descent
  * clipped gradient descent
  * change of variables
* They use the results of these combinations to narrow down the best attacks for the L_0, L_2, and L_inf norm
* They are able to beat existing defenses with 100% effectiveness with all three attacks in all 3 targeted attack scenarios
* Note from researchers:
  * We encourage those who create defenses to perform the two evaluation approaches we use in this paper:
      * Use a powerful attack (such as the ones proposed in this paper) to evaluate the robustness of the secured model directly. Since a defense that prevents our L2 attack will prevent our other attacks, defenders should make sure to establish robustness against the L2 distance metric
      * Demonstrate that transferability fails by constructing high-confidence adversarial examples on an unsecured
model and showing they fail to transfer to the secured model


## [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)
* They look at various optimization problems with a min-max formulation to motivate their creation of the Projected Gradient Descent (PGD) algorithm, which creates the "the strongest attack utilizing the local first order information about the network."   
   * The inner maximization problem aims to find an adversarial version of a given data point x that achieves a high loss (aka attack the network)
   * The goal of the outer minimization problem is to find model parameters so that the “adversarial loss” given by the inner attack problem is minimized (aka adversarial training)
* They also correlate adversarial robustness with the capacity of the model and robustness in the decision boundaries of the min-max problem
* They then use this algorithm to perform adversarial training on CIFAR-10 and MNIST models to improve their robustness to a wide range of adversarial attacks
* They focus on finding an algorithm for L_inf
* Observe that the loss achieved by the adversary increases in a fairly consistent way and plateaus rapidly when performing projected `∞ gradient descent for randomly chosen starting points inside x + S
* experiments show that the local maxima found by PGD all have similar loss values, both for normally trained networks and adversarially trained networks. This concentration phenomenon suggests the PGD adversary yields robustness against all first-order adversaries
   * "first-order" in the context of adversarial attacks refers to the use of the gradient of the model's loss function with respect to the input data
* argues PGD adversarial training also reduces transferability
     
<img width="436" alt="image" src="https://github.com/christymarc/thesis/assets/70297740/be1e10f1-1cd0-495f-a879-2530b3ae7e76">


