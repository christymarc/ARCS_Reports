# Activities
* Implemented FSGD method of adversarial example generation
* read paper where FSGD was introduced

# Issues
* Need wandb walkthrough -- how to use in code
* GPU issue:
  ![image](https://github.com/christymarc/ARCS_Reports/assets/70297740/6ace5b7b-4ac0-4a7f-8a64-38cb0ba64747)
  * solution: GPU reboot

# Plans
* continue implementing base methods
* put in time measurements for generation as well

# Paper Summaries
## [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf)
* Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples
* Show that adversarial training can provide an additional regularization benefit
* Adversarial examples show that models are not learning the true underlying concepts that determine the correct output label. Instead, these algorithms have built a Potemkin village that works well on naturally occurring data, but is exposed as a fake when one visits points in space that do not have high probability in the data distribution
* For high-dimensional problems, we can make many infinitesimal changes to the input that add up to one large change to the output. We can think of this as a sort of “accidental steganography,” where a linear model is forced to attend
exclusively to the signal that aligns most closely with its weights, even if multiple signals are present and other signals have much greater amplitude
* This form of data augmentation instead uses inputs that are unlikely to occur naturally but that expose flaws in the ways that the model conceptualizes its decision function
* The adversarial training procedure can be seen as minimizing the worst-case error when the data is perturbed by an adversary
*  Our view of adversarial training is that it is only clearly useful when the model has the capacity to learn to
resist adversarial examples. This is only clearly the case when a universal approximator theorem applies (aka as long at the hidden layer has enough nodes to represent all the features, NNs should be able to approximate any function)
* The generalization of adversarial examples across different models can be explained as a result of adversarial perturbations being highly aligned with the weight vectors of a model, and different models learning similar functions when trained to perform the same task.
* The direction of perturbation, rather than the specific point in space, matters most. Space is not full of pockets of adversarial examples that finely tile the reals like the rational numbers
