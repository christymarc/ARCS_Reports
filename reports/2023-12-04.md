# Activities

# Issues

# Goals

# Paper Summaries
## [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)
* Created the targeted evasion attack CW
* Three approaches in targeted attack (targeted attack targets certain classes while untargeted just aim for the class to be incorrect):
  * Average Case: select the target class uniformly at random among the labels that are not the correct label.
  * Best Case: perform the attack against all incorrect classes, and report the target class that was least difficult to attack.
  * Worst Case: perform the attack against all incorrect classes, and report the target class that was most difficult
to attack.
* They use 3 distance metrics (all L norms) Distance metrics are usually used in order to limit the potential noticeable perturbation to a human viewer.
  * L_0 -> number of pixels unaltered in the formation of an adversarial example
  * L_2 -> Euclidean norm between the adversary example and the un-perturbed image
  * L_inf -> measures max change of any of the pixels between the adversary example and the un-perturbed image
* Tested it on MNIST, CIFAR-10, ImageNet
  
  <img width="263" alt="image" src="https://github.com/christymarc/thesis/assets/70297740/e17b3099-c991-458a-ad63-aad758a27263">

* for imagenet, used the pre-trained Inception v3 network which achieves 96% top-5 accuracy
* They look at a variety of functions that meet the constraints of their objective function f such that C(x + δ) = t if and only if f(x + δ) ≤ 0.
* Then they consider these functions on three different methods of box constraint (aka constraint on δ: we must have 0 ≤ xi +δi ≤ 1 for all i):
  * projected gradient descent
  * clipped gradient descent
  * change of variables
* They use the results of these combinations to narrow down the best attacks for the L_0, L_2, and L_inf norm
* They are able to beat existing defenses with 100% effectiveness with all three attacks in all 3 targeted attack scenarios
* Note from researchers:
  * We encourage those who create defenses to perform the two evaluation approaches we use in this paper:
      * Use a powerful attack (such as the ones proposed in this paper) to evaluate the robustness of the secured model directly. Since a defense that prevents our L2 attack will prevent our other attacks, defenders should make sure to establish robustness against the L2 distance metric
      * Demonstrate that transferability fails by constructing high-confidence adversarial examples on an unsecured
model and showing they fail to transfer to the secured model
