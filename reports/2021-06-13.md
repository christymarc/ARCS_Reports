# Activities

* Edited the output of the pytorch model when training to include times and when going through the validation set to include class accuracy
* Put everything on the GPU to increase training speed
* Fixed pytorch model validation set (using argmax instead of torch.max)
* Tested different learning rates
* Added initial weights to pytorch model
* Fixed issue with splitting the dataset in pytorch model
* Made models with different hidden layers (dropout layers/batch normalization)
  * Tested how the layers affected performance in the maze
* Read OpenBot paper
* Began reading paper on dropout layers

# Issues

* Server issues this week
* Still having issues with cnn_learner for fastai model

# Plans

* Need to train models on more image data
  * Generate more mazes, use edited autogen to generate more image data with proper labels
* Try out different resnets
* Try out different initial weights
* Try to get fastai model working better
* Interested in maybe trying some data augmentation, differing lighting, etc.
* Continue working through fastai course

# Article Summaries

[OpenBot: Turning Smartphones Into Robots](https://arxiv.org/pdf/2008.10631.pdf)

Researchers Muller and Koltun addressed the problem of accessibility in robot development, seeking to reduce the cost of development without sacrificing computational ability and comprehensive sensor architecture. Their approach to this problem involved using Android smartphones with extensive sensor suites, computational capabilities, and already established software ecosystems to reduce the cost of purchasing sensors, GPUs/CPUs, batteries, and displays separately. Ultimately, they were able to develop a wheeled robot with navigation and person-following capabilities for $50 plus the cost of a used Android smartphone. Their robot's software stack consisted of an Arduino for measuring wheel speed, batter voltage, etc. and an Android application which collected datasets (of images, GPS coordinates, and other sensor readings), provided an interface for remote control via a PS4/Xbox controller, and ran neural network models, using the Tensorflow Lite infrastructure, on the phones' mobile GPUs while also obtaining the output data.  In obtaining a dataset for training the robot's model, they used the game controller, getting image data and control labeling. 
