# Activities
* Lit review for paper
* Met with Liz to discuss plans with next paper
* scheduled check-in for the lab

# Issues
* N/A

# Plans
* Research conferences for publications
* Continue lit review

# Research Article Summaries
## Sim-to-Real
### [Connecting the Digital and Physical World: Improving the Robustness of Adversarial Attacks](https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.3301962)
* Goal: explore the feasibility of generating robust adversarial examples that remain effective in the physical domain; address the more realistic attack space where an attacker may print out an image in order to attack the network deployed on a device or robot and account for transformations created by an attacker's printer, camera quality, etc.
* Novel method D2P -> generate robust adversarial examples that can survive in the physical world. The core idea is to explicitly simulate the digital-to-physical transformation of the physical devices (e.g., paper printing, non-linear camera response functions, sensor quantization, and noises) to translate a digital image to its simulated physical version and then generate adversarial noise on top of that image 
  * image-to-image translation layer ->  conditional Generative Adversarial Networks (cGAN) to simulate this process with pix2pix and cycleGAN
  * Exception over Transformation (EOT) method to generate adversarial noise,  aiming to misclassify the image as the least-likely label
    * to address the problem of adversarial examples that target a certain adversarial class in the label space not maintaining their effectiveness when angles change, etc.
    * "The key insight behind EOT is to model such perturbations within the optimization procedure. Rather than optimizing the log-likelihood of a single example, EOT uses a chosen distribution T of transformation functions t taking an input x0 controlled by the adversary to the “true” input t(x0) perceived by the classifier. Furthermore, rather than simply taking the norm of x0 − x to constrain the solution space, given a distance function d(·, ·), EOT instead aims to constrain the expected effective distance between the adversarial and original inputs..." [Synthesizing Robust Adversarial Examples](https://arxiv.org/pdf/1707.07397.pdf)
  * 3000+ physical-domain images to assess strengths and weaknesses
* For each image, print it out using a printer and retake the photo of the printout using a camera at the frontal view then perform multiple rounds of transformation (photos at diff angles) and measure the image similarity (or dissimilarity) to the original clean image
* used 200 training images randomly selected from ImageNet’s validation set. Printed each image and then re-take the photo to obtain its
physical version (Canon PIXMA TS9020 printer and iPhone 6s camera), and used this dataset with paired digital and physical images to facilitate the training
* Results:
  * the digital versions of the adversarial images are highly successful
  * adversarial examples are more difficult to succeed in the physical domain
  * both of D2P models outperform existing methods by a large margin. Compared to EOT and RP2,the  method significantly improves the target label’s ranking.
  * D2Pp slightly outperforms D2Pc in the attacking results. D2Pc uses the cycleGAN for learning the digital-to-physical transformation.
  * digital-to-physical model was trained only using the front view images but results shows that generalizes well (compared to EOT) to other previously unseen situations
  * different printer or camera does not significantly affect the results
 
* Note: guide to table 3
  * digital side (digital version of the adversarial images):
    * left -> the true labels likelihood of classification, top1, top5
    * right -> the most adversarial labels likelihood of "
  * physical side (printed out image of the adversarial images):
    * left, right -> "  
* Questions:
  * how were images collected (and how many were used) to train and test those networks to transition digital-to-physical (was the total that 200 number?)? Imagine it's pretty labor-intensive to take a photo of every image in your dataset and associate it with its digital equivalent so wondering how much work was there.
  * "Note that unlike pix2pix, cycleGAN does not require “paired” images for training, which can tolerate the potential misalignment between the digital and physical images" - how can it facilitate this transition without paired images???
  * what is meant when you say your "method is operated by digital simulations"? Just meaning you are simulating a physical space of transition between the digital and physical rather than a real transition between these?
  * what is D2P physical?

