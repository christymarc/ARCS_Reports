# Activities
* Lit review for paper
* Met with Liz to discuss plans with next paper
* scheduled check-in for the lab

# Issues
* N/A

# Plans
* Research conferences for publications
* Continue lit review

# Research Article Summaries
## Sim-to-Real
### [Connecting the Digital and Physical World: Improving the Robustness of Adversarial Attacks](https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.3301962)
* Goal: explore the feasibility of generating robust adversarial examples that remain effective in the physical domain; address the more realistic attack space where an attacker may print out an image in order to attack the network deployed on a device or robot and account for transformations created by an attacker's printer, camera quality, etc.
* Novel method D2P -> generate robust adversarial examples that can survive in the physical world. The core idea is to explicitly simulate the digital-to-physical transformation of the physical devices (e.g., paper printing, non-linear camera response functions, sensor quantization, and noises) to translate a digital image to its simulated physical version and then generate adversarial noise on top of that image 
  * image-to-image translation layer ->  conditional Generative Adversarial Networks (cGAN) to simulate this process with pix2pix and cycleGAN
  * Exception over Transformation (EOT) method to generate adversarial noise,  aiming to misclassify the image as the least-likely label
    * to address the problem of adversarial examples that target a certain adversarial class in the label space not maintaining their effectiveness when angles change, etc.
    * "The key insight behind EOT is to model such perturbations within the optimization procedure. Rather than optimizing the log-likelihood of a single example, EOT uses a chosen distribution T of transformation functions t taking an input x0 controlled by the adversary to the “true” input t(x0) perceived by the classifier. Furthermore, rather than simply taking the norm of x0 − x to constrain the solution space, given a distance function d(·, ·), EOT instead aims to constrain the expected effective distance between the adversarial and original inputs..." [Synthesizing Robust Adversarial Examples](https://arxiv.org/pdf/1707.07397.pdf)
  * 3000+ physical-domain images to assess strengths and weaknesses
* For each image, print it out using a printer and retake the photo of the printout using a camera at the frontal view then perform multiple rounds of transformation (photos at diff angles) and measure the image similarity (or dissimilarity) to the original clean image
* used 200 training images randomly selected from ImageNet’s validation set. Printed each image and then re-take the photo to obtain its
physical version (Canon PIXMA TS9020 printer and iPhone 6s camera), and used this dataset with paired digital and physical images to facilitate the training
* Results:
  * the digital versions of the adversarial images are highly successful
  * adversarial examples are more difficult to succeed in the physical domain
  * both of D2P models outperform existing methods by a large margin. Compared to EOT and RP2,the  method significantly improves the target label’s ranking.
  * D2Pp slightly outperforms D2Pc in the attacking results. D2Pc uses the cycleGAN for learning the digital-to-physical transformation.
  * digital-to-physical model was trained only using the front view images but results shows that generalizes well (compared to EOT) to other previously unseen situations
  * different printer or camera does not significantly affect the results
 
* Note: guide to table 3
  * digital side (digital version of the adversarial images):
    * left -> the true labels likelihood of classification, top1, top5
    * right -> the most adversarial labels likelihood of "
  * physical side (printed out image of the adversarial images):
    * left, right -> "  
* Questions:
  * how were images collected (and how many were used) to train and test those networks to transition digital-to-physical (was the total that 200 number?)? Imagine it's pretty labor-intensive to take a photo of every image in your dataset and associate it with its digital equivalent so wondering how much work was there.
  * "Note that unlike pix2pix, cycleGAN does not require “paired” images for training, which can tolerate the potential misalignment between the digital and physical images" - how can it facilitate this transition without paired images???
  * what is meant when you say your "method is operated by digital simulations"? Just meaning you are simulating a physical space of transition between the digital and physical rather than a real transition between these?
  * what is D2P physical?

## RoboTHOR: An Open Simulation-to-Real Embodied AI Platform
* Goal: democratize research in interactive and embodied visual AI by attempting to create a simulation that helps bridge the reality gap
* ROBOTHOR offers a framework of simulated environments paired with physical counterparts to systematically explore and overcome the challenges of simulation-to-real transfer, and a platform where researchers across the globe can remotely test their embodied models in the physical world.
* Scenes. ROBOTHOR consists of a set of 89 apartments, 75 in train/val (we use 60 for training and 15 for validation), 4 in test-dev (which are used for validation in the real world) and 10 in test-standard (blind physical test set) drawn from a set of 15, 2 and 5 wall layouts respectively.
* In total there are 731 unique object instances in the asset library with no overlap among train/val, test-dev and test-standard scenes.
* baseline models:
  * Random - This model chooses an action randomly amongst the set of possible actions
  * Instant Done - This model invokes the Done action at the every first time step. The purpose of this baseline is to measure the percentage of trivial starting locations.
  * Blind - This model receives no sensory input. It only consists of an LSTM with simply a target embedding input.
  * Image-A3C - The image is fed into a pre-trained and frozen ResNet-18 to obtain a 7 × 7 × 512 tensor, followed by two 1×1 convolution layers to reduce the channel depth to 32 and finally concatenated with a 64-dimensional embedding of the target word and provided to an LSTM which generates the policy. The agent is trained using the asynchronous advantage actor-critic (A3C) [40] formulation, to act to maximize its expected discounted cumulative reward. Two kinds of rewards are provided: a positive success reward and a negative step reward to encourage efficient paths.
  * Image+Detection-A3C - The image is fed into Faster RCNN [48] trained on the MSCOCO dataset.
* metrics: success rate + path length
* sim-to-sim results: The trivial baselines Random, Instant Done and Blind perform very poorly, indicating that the dataset lacks a bias that is trivial to exploit using these models. The Image only model performs reasonably well, succeeding at over 50% of easy trajectories but doing very poorly at hard ones. Adding object detection does not help performance.
* sim-to-real results: sizeable drop in performance for the real robot
* also tried real to sim image translation (like Gibson) but it did not work

