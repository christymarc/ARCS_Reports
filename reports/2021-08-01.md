# Activities

* Read Papers (see summaries below of ones I thought were notable)
* Wrote part of the methods section in paper
* Added code to BasicClassification script and began running it
 * Code to ensure models aren't trained if they already exist in the directory

# Issues

* Power outage on Friday potentially delayed model training

# Plans

* Continue reading papers and searching for new ones
* Continue writing the paper
* Write script to train cmd models and RNN models

# Article Summaries

## Comparison and benchmarking of AI Models and Frameworks on Mobile Devices

* Note: I think this paper is a good reference for describing different network architectures (good visual and textual descriptions): ResNet, Inception, DenseNet, SqueezeNet. Also good to see how their comparison of these architectures compares with our comparison.

Summary: The researchers developed AIoTBench a benchmark suite which evaluates the performances of 6 different network architectures developed through 3 different deep learning frameworks and deployed on 5 different mobile devices. The networks were evaluated by the metric of accuracy, valid images per second (VIPS) (a user level metric of how many images can be processed per second), and valid FLOPS per second (VOPS) (a system-level metric of how much the system can compute per second). The networks were tasked with image classification, trained on 1,280,000 images with 1000 classes, and validated on 5000 images (5 from each class). The paper gives a good summary of each network architecture they used including how they differ from the other architectures. The data was normalized, channels were rearranged per architecture, and images were resized. The researchers found taht across different mobile platforms, performance and accuracy differed. They also found that different AI framworks worked differently on each device and that there wasn't one definitiive answer for which network worked the best. Accuracy wise, the ranking went (from best to worst): InceptionV3, Resnet50, MnasNet, DenseNet121, MobileNetV2, and SqueezeNet (this one doing significantly worse than the others). 

## Benchmark Analysis of Representative Deep Neural Network Architectures

* Note: This paper also compares the performance of DNN architectures, including VGG, SqueezeNet, ResNet, Inceptions, DenseNet, ResNeXt, SE-ResNet, and SE-ResNeXt. It may be helpful to see how our comparison differs with this one.

Summary: The researchers set out to compare 40 different DNN architectures on the basis of accuracy rate, model complexity, memory usage, computational complexity, and inference time. The data used was from the ImageNet-1k challenge; therefore, these models were given an image classification task.

<img width="533" alt="Bench_DNN_Graph" src="https://user-images.githubusercontent.com/70297740/127369402-83c7e4de-dbd7-42bd-8cde-450cb90849f3.PNG">

Research takeaways: 
* There's no correlation between model complexity and recognition accuracy.
* The models that use their parameters the most efficiently (by the metric of accuracy divided by the number of parameters) are the squeezeNets, ShuffleNets, MobileNets, and NASNet-A-Mobile. Not all models use their parameters to teh same level of efficiency.
* See graph for model comparison

## Electric Load Forecasting in Smart Grid Using Long-Short-Term-Memory based Recurrent Neural Network

* Note: I think this paper would be helpful if we wanted to get into the details of how LSTMs work prior to explaining how/why we used them in this research. This paper goes into detail about why LSTMs are helpful for time series with nonlinear characteristics, not image classification, but they have a very detailed explanation on why LSTMs are special and how they work mathematically. 

Summary: The researchers explored the use of LSTM to predict nonlinear, nonstationary, and nonseasonal electric load time series data. LSTMs are able to exploit the long term dependencies in the electric load time series for more accurate forecasting. When compared to other load forecasting methods like SVR, NNETAR, NARX, and SARIMA for the task of forecasting electricity consumption, the LSTM network did significantly better.

How the paper described LSTMs: 
* LSTM introduces the concepts of gate and memory cells in each hidden layer.
* LSTMs consist of memory blocks which have four parts: an input gate (which controls the entry of the activations to the memory cell), a forget gate (whcih helps the network to forget the past input data, get rid of data it doesn't need, and reset the memory cells when necessary), and output gate (which learns when to output the activations to the successive network), and self-connected memory cells (which allow the network to access and store info over a long time interval).
* LSTMs are an architecture suitable for problems with long-term dependencies (in the real world or a real maze, you'd probably want to remember where you've been so you don't repeat your steps).
* Math, math, math

 
